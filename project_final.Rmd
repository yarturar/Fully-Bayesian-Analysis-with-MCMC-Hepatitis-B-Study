---
title: |
  | \vspace{6cm} \LARGE\textbf{SDS II Project: Hepatitis B Study}
header-includes:
- \usepackage{titling}
- \pretitle{\begin{center}\LARGE\includegraphics[width=7cm]{C:/Users/admin/Desktop/logo.png}\\[\bigskipamount]}
- \posttitle{\end{center}}
author: "MADIYAR TURAR"
date: '30 March 2019'
output: 
  pdf_document:
    fig_caption: true
   
  
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

\newpage 
\tableofcontents 

\newpage


##Introduction 
In this project, we are going to consider The Gambian Hepatitis Intervention Study (GHIS). This study is gambian national vaccination programm to reduce Hepatitis B (HB) carriers rate in population. This project aims to analyze a level of surface-antibody of HB, which is called anti-HB-titre, over time. Other studies found that after vaccination the level of HB antibody decreases with following realationship (Coursaget et al. 1991):
$$anti-HB-titre  \sim \frac{1}{time}$$
where $time$ is time since final vaccination. Rewriting the formula above in log:
$$log(anti-HB-titre) = \alpha_i - log(time)$$
here, $\alpha_i$ is constant after final dose of vaccination and it differs for each infant i. However, gradient of $log(time)$, which is -1, was found to be common for all infants according to Coursaget study. So, in this project we are going to validate finding of Coursaget for $\alpha_i$ and gradient $-1$. 


## Data 

The GHIS data contains HB measurement data of 106 children. After HB vaccination, blood samples from each infants were collected for checking level of anti-HB-titre. Overall, three HB tests were done with approximate interval of six months between tests. The first measurement was done right after vaccination. At the first and second test, blood samples from all 106 children were tested, and at the third test only 76 children were checked. So, there are overall 288 data points for HB (Yvec1), which is measured in milli-International-Units, and for time (tvec1),measured as days, and both are converted into log-scale. 

|   **Data,log scale**  |
|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|
| $Infant ID$ | $time_1$ | $HBtitre_1$ | $time_2$ | $HBtitre_2$ | $time_3$ | $HBtitre_3$ |
| 1 | 6.54 | 5.00 | 6.96 | 8.03 | 10 | NA |
| 2 | 5.84 | 6.83 | 6.53 | 4.91 | 6.98 | 6.30 |
| 3 | 6.60 | 3.95 | 7.02 | 4.35 | 10 | NA |
| ... | ... | ... | ... | ... | ... | ... |
| 106 | 5.86 | 9.09 | 6.92 | 7.94 | 10 | NA |




Data can be found by this link: https://github.com/stan-dev/example-models/blob/master/bugs_examples/vol3/hepatitis/hepatitis.data.R

First of all, we will plot raw data of 106 GHIS infants with seperate line for each infants anti-HB-titre measurement. Furthermore, average value based straight line was added into the plot.

```{r, include=FALSE}
#the dataset 
#number of infants:
N <-
  106

#anti HB titre measurement:
Yvec1 <-
  c(4.997, 6.83, 3.95124371858143, 6.79794041297493, 4.71849887129509, 
    5.78996017089725, 6.13122648948314, 6.76272950693188, 6.14846829591765, 
    10.7579879834538, 5.63121178182137, 3.46573590279973, 7.92479591395644, 
    9.16115012778541, 8.74257423767064, 5.91079664404053, 6.54678541076052, 
    2.94443897916644, 7.05272104923232, 0, 9.13905916997122, 8.48260174664662, 
    7.11963563801764, 7.64300363556072, 9.42294862137501, 8.2553088117856, 
    4.82028156560504, 6.54678541076052, 8.67470962929122, 7.86326672400957, 
    4.67282883446191, 7.41878088275079, 6.84587987526405, 5.46383180502561, 
    3.43398720448515, 7.7553388128465, 9.04239498112674, 8.45212119467252, 
    7.91425227874244, 7.72885582385254, 9.31433996199467, 6.35957386867238, 
    2.30258509299405, 8.29354951506035, 8.90082160491523, 8.87905466204227, 
    11.5129154649202, 8.79618763547045, 8.40155784781731, 8.89370997756851, 
    7.1800698743028, 4.82028156560504, 5.77765232322266, 6.4707995037826, 
    4.40671924726425, 8.17413934342947, 2.30258509299405, 9.77104128523582, 
    6.95081476844258, 5.97635090929793, 5.8805329864007, 5.73979291217923, 
    5.75574221358691, 5.15329159449778, 8.11761074646623, 8.54636356871602, 
    7.58171964012531, 6.9037472575846, 4.66343909411207, 7.47986413116503, 
    3.61091791264422, 4.64439089914137, 4.39444915467244, 8.23509549725836, 
    6.12905021006055, 8.4424696452203, 6.88755257166462, 7.29233717617388, 
    6.38856140554563, 8.75998249497728, 6.08677472691231, 4.02535169073515, 
    4.38202663467388, 6.85540879860993, 8.17919979842309, 4.24849524204936, 
    8.98343977178426, 8.4721958254855, 9.480214825778, 6.1463292576689, 
    9.93542217106647, 4.27666611901606, 5.36597601502185, 4.60517018598809, 
    3.2188758248682, 6.81783057145415, 6.58479139238572, 5.91889385427315, 
    8.67641669696422, 2.484906649788, 6.73933662735717, 3.91202300542815, 
    4.56434819146784, 9.15925758174687, 7.07326971745971, 9.08636319215647, 
    8.028, 4.905, 4.35670882668959, 5.27299955856375, 3.49650756146648, 
    3.68887945411394, 3.85014760171006, 3.29583686600433, 5.25749537202778, 
    10.4925789214972, 4.20469261939097, 1.79175946922805, 7.54750168281497, 
    8.34236350038058, 7.07580886397839, 5.11198778835654, 4.93447393313069, 
    3.17805383034795, 6.25382881157547, 7.19067603433221, 7.82724090175281, 
    7.51697722460432, 5.8348107370626, 6.35610766069589, 8.57073395834427, 
    7.16394668434255, 3.40119738166216, 5.95583736946483, 7.32383056620232, 
    7.63385355968177, 3.36729582998647, 6.76157276880406, 8.38845031552351, 
    9.9219166880045, 1.6094379124341, 7.44366368311559, 6.64768837356333, 
    6.85118492749374, 6.16961073249146, 9.75214127004149, 8.26359043261732, 
    5.84643877505772, 0, 7.60589000105312, 6.25958146406492, 8.88391747120797, 
    8.54286093816481, 7.86095636487639, 6.91075078796194, 7.59588991771854, 
    7.03174125876313, 2.99573227355399, 3.91202300542815, 5.4510384535657, 
    5.12989871492307, 7.88419993367604, 2.30258509299405, 9.32892308780313, 
    10.5472081164337, 5.71042701737487, 6.50128967054039, 5.53338948872752, 
    4.99043258677874, 3.61091791264422, 7.34729970074316, 8.03138533062553, 
    6.92264389147589, 6.7202201551353, 3.43398720448515, 6.19236248947487, 
    3.78418963391826, 4.51085950651685, 3.55534806148941, 7.81116338502528, 
    6.19847871649231, 3.55534806148941, 5.07517381523383, 7.25417784645652, 
    6.21460809842219, 7.19593722647557, 4.66343909411207, 3.78418963391826, 
    2.484906649788, 6.08221891037645, 6.04500531403601, 3.52636052461616, 
    9.69990150043745, 7.76004068088038, 7.78113850984502, 6.289715570909, 
    9.39806397805913, 5.34710753071747, 5.88887795833288, 5.29831736654804, 
    4.27666611901606, 3.46573590279973, 7.2152399787301, 5.68357976733868, 
    7.72179177681754, 1.38629436111989, 6.73221070646721, 5.4971682252932, 
    2.83321334405622, 8.58073121222023, 7.12608727329912, 7.9359451033537, 
    6.295, 4.31748811353631, 4.29045944114839, 3.43398720448515, 
    5.37527840768417, 3.49650756146648, 2.77258872223978, 6.40687998606931, 
    7.9483852851119, 6.3919171133926, 5.37989735354046, 5.86929691313377, 
    5.34233425196481, 4.45434729625351, 6.5424719605068, 6.89365635460264, 
    8.79026911147866, 7.46278915741245, 3.66356164612965, 4.99721227376411, 
    8.03040956213048, 7.16006920759613, 3.17805383034795, 6.31535800152233, 
    2.77258872223978, 7.82803803212583, 7.11151211649616, 7.01481435127554, 
    6.00881318544259, 9.59703032475801, 2.30258509299405, 10.1017644761202, 
    7.06561336359772, 8.97575663051942, 7.08757370555797, 7.22256601882217, 
    2.07944154167984, 2.70805020110221, 3.61091791264422, 3.85014760171006, 
    7.22766249872866, 0, 9.61767040669386, 8.66905554072548, 5.24174701505964, 
    5.48479693349065, 4.04305126783455, 6.55250788703459, 7.35115822643069, 
    6.52502965784346, 5.74939298590825, 3.09104245335832, 2.99573227355399, 
    3.29583686600433, 7.54855597916987, 4.81218435537242, 2.94443897916644, 
    5.03043792139244, 2.39789527279837, 2.94443897916644, 5.45958551414416, 
    3.58351893845611, 8.74097653801779, 5.87493073085203, 9.17294998275762, 
    3.46573590279973, 3.36729582998647, 3.58351893845611, 7.63867982387611, 
    5.90808293816893, 5.8171111599632, 6.19031540585315, 2.56494935746154, 
    4.51085950651685, 4.39444915467244, 5.01063529409626)

#id of infants at each measurment:
idxn1 <-
  c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 
    15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 
    28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 
    41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 
    54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 
    67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 
    80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 
    93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 
    105, 106 , 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 
    13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 
    26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 
    39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 
    52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 
    65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 
    78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 
    91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 
    103, 104, 105, 106, 2, 4, 5, 6, 9, 11, 12, 13, 14, 
    15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 
    31, 33, 35, 36, 37, 38, 39, 40, 43, 44, 45, 47, 48, 
    49, 51, 52, 53, 55, 56, 57, 58, 59, 60, 61, 62, 65, 
    66, 67, 68, 69, 72, 73, 74, 75, 76, 79, 81, 82, 84, 
    86, 87, 90, 91, 92, 93, 94, 95, 97, 98, 99, 100, 
    101, 102, 105)

#time measured:
tvec1 <-
  c(6.541, 5.8406416573734, 6.60123011872888, 5.8636311755981, 
    5.91350300563827, 5.91889385427315, 5.87493073085203, 5.82008293035236, 
    5.91350300563827, 5.98896141688986, 5.95064255258773, 5.75574221358691, 
    5.92157841964382, 5.93489419561959, 5.91350300563827, 5.97126183979046, 
    5.97126183979046, 6.59987049921284, 5.92157841964382, 6.71780469502369, 
    5.92958914338989, 5.92157841964382, 5.92157841964382, 5.92157841964382, 
    5.92157841964382, 5.8406416573734, 5.89440283426485, 5.93489419561959, 
    5.93224518744801, 5.96870755998537, 5.8805329864007, 6.62671774924902, 
    5.87773578177964, 6.59030104819669, 5.92958914338989, 5.92958914338989, 
    5.86078622346587, 5.89440283426485, 5.95064255258773, 5.93224518744801, 
    5.97126183979046, 6.57088296233958, 5.93753620508243, 5.87493073085203, 
    5.89164421182577, 6.74170069465205, 5.89440283426485, 5.94803498918065, 
    5.77455154554441, 5.97126183979046, 5.96870755998537, 5.8805329864007, 
    5.89440283426485, 6.62936325343745, 5.93224518744801, 5.99146454710798, 
    5.88887795833288, 5.93489419561959, 5.94017125272043, 5.95583736946483, 
    5.93489419561959, 5.92692602597041, 6.64639051484773, 5.93224518744801, 
    5.96870755998537, 5.93224518744801, 5.93224518744801, 5.93224518744801, 
    5.94542060860658, 5.86929691313377, 6.65801104587075, 5.89715386763674, 
    6.00881318544259, 5.97126183979046, 5.97126183979046, 5.97126183979046, 
    6.00881318544259, 6.64639051484773, 5.88610403145016, 5.75257263882563, 
    5.68017260901707, 5.77144112313002, 5.96100533962327, 5.75257263882563, 
    5.81413053182507, 5.63478960316925, 5.77455154554441, 5.77765232322266, 
    5.63121178182137, 5.70711026474888, 5.89164421182577, 5.77455154554441, 
    6.0591231955818, 5.8171111599632, 5.65248918026865, 5.60947179518496, 
    5.83773044716594, 5.91350300563827, 5.77455154554441, 5.83773044716594, 
    5.98645200528444, 5.8171111599632, 6.49526555593701, 5.68697535633982, 
    5.8171111599632, 5.85793315448346, 6.963, 6.52941883826223, 7.02553831463852, 
    6.51767127291227, 6.5694814204143, 6.5694814204143, 6.97447891102505, 
    6.56244409369372, 6.5510803350434, 7.0335064842877, 6.59987049921284, 
    6.65544035036765, 6.70686233660275, 6.63987583382654, 6.63987583382654, 
    6.55961523749324, 6.60123011872888, 7.00215595440362, 6.68959926917897, 
    6.98286275146894, 6.56807791141198, 6.71780469502369, 6.54821910276237, 
    6.54821910276237, 6.54821910276237, 6.54821910276237, 6.49072353450251, 
    6.70686233660275, 6.56103066589657, 6.52795791762255, 6.56244409369372, 
    6.94889722231331, 6.54965074223381, 7.00669522683704, 6.56807791141198, 
    6.56807791141198, 6.55819780281227, 6.59030104819669, 6.51767127291227, 
    6.70196036600254, 6.60934924316738, 7.01571242048723, 6.5206211275587, 
    6.56103066589657, 6.59030104819669, 6.98933526597456, 6.5410299991899, 
    6.5510803350434, 6.49828214947643, 6.54965074223381, 6.58892647753352, 
    6.59030104819669, 6.5410299991899, 6.98471632011826, 6.62007320653036, 
    6.63856778916652, 6.59441345974978, 6.61873898351722, 6.62140565176413, 
    6.62936325343745, 6.61873898351722, 6.61204103483309, 7.04403289727469, 
    6.62007320653036, 6.64639051484773, 6.64768837356333, 6.62007320653036, 
    6.62671774924902, 6.62140565176413, 6.5875500148248, 7.05875815251866, 
    6.59714570188665, 6.62936325343745, 6.66568371778241, 6.66440902035041, 
    6.66440902035041, 7.05185562295589, 7.04403289727469, 6.59167373200866, 
    6.96129604591017, 6.50876913697168, 6.56244409369372, 6.66695679242921, 
    6.49828214947643, 7.02108396428914, 6.46769872610435, 6.48768401848461, 
    6.52209279817015, 6.55535689181067, 6.45833828334479, 6.48920493132532, 
    6.48920493132532, 6.62671774924902, 6.49223983502047, 6.42162226780652, 
    6.44413125670044, 6.48616078894409, 6.52649485957079, 6.45047042214418, 
    6.48463523563525, 6.51025834052315, 6.48463523563525, 6.94215670569947, 
    6.47850964220857, 6.51174532964473, 6.92264389147589, 6.98193467715639, 
    6.96790920180188, 7.00850518208228, 7.00760061395185, 7.00306545878646, 
    7.00940893270864, 6.9555926083963, 6.98286275146894, 6.9782137426307, 
    6.9782137426307, 7.00124562206948, 7.00033446027523, 6.96224346426621, 
    6.98286275146894, 6.98193467715639, 6.98193467715639, 6.98100574072173, 
    6.98100574072173, 6.95654544315157, 7.00306545878646, 7.0335064842877, 
    6.94215670569947, 6.97728134163075, 6.98841318199959, 6.98193467715639, 
    6.98193467715639, 6.97447891102505, 6.98378996525813, 6.97447891102505, 
    6.97447891102505, 6.97073007814353, 6.97447891102505, 6.98378996525813, 
    6.98286275146894, 6.99668148817654, 6.95844839329766, 7.01481435127554, 
    6.98378996525813, 6.98286275146894, 7.03174125876313, 7.04053639021596, 
    7.02908756414966, 7.04577657687951, 7.05272104923232, 7.05272104923232, 
    7.04490511712937, 7.04141166379481, 7.03174125876313, 7.028201432058, 
    7.03174125876313, 7.02731451403978, 7.05272104923232, 7.03174125876313, 
    7.03966034986208, 7.03878354138854, 7.04403289727469, 7.04403289727469, 
    7.028201432058, 6.97541392745595, 6.99759598298193, 6.99025650049388, 
    6.95081476844258, 6.92067150424868, 6.91473089271856, 6.92165818415113, 
    6.92165818415113, 7.01391547481053, 6.94119005506837, 6.86380339145295, 
    6.92165818415113, 6.97447891102505, 6.92755790627832, 6.91869521902047, 
    6.96413561241824, 6.94793706861497, 6.93634273583405)

#first measurement of HBtitre
y0 <-
  c(8.613, 7.105, 6.896, 5.63835466933375, 6.35088571671474, 5.03043792139244, 
    5.8664680569333, 5.73979291217923, 7.07834157955767, 8.56655462095396, 
    6.86171134048073, 6.97354301952014, 7.34213173058472, 8.82629423124132, 
    6.48768401848461, 6.66440902035041, 7.0343879299155, 4.49980967033027, 
    6.85118492749374, 3.04452243772342, 8.63905677917308, 1.79175946922805, 
    7.01660968389422, 7.09090982207998, 7.42356844425917, 8.09620827165004, 
    5.70711026474888, 7.01031186730723, 7.18311170174328, 8.15219801586179, 
    5.99645208861902, 7.0825485693553, 5.06890420222023, 7.04228617193974, 
    4.0943445622221, 7.32052696227274, 7.12689080889881, 7.24136628332232, 
    6.62007320653036, 7.36897040219479, 8.47720418319987, 7.20340552108309, 
    2.89037175789616, 7.39141523467536, 6.25190388316589, 8.38160253710989, 
    11.5129154649202, 6.60258789218934, 7.61283103040736, 7.42476176182321, 
    6.71174039505618, 5.56068163101553, 6.0282785202307, 7.10085190894405, 
    7.52671756135271, 7.92948652331429, 1.6094379124341, 6.85856503479136, 
    8.05006542291597, 5.83188247728352, 7.15851399732932, 7.01481435127554, 
    7.70120018085745, 7.16239749735572, 5.99893656194668, 5.41610040220442, 
    8.40782465436087, 7.18462915271731, 3.25809653802148, 8.14380797677148, 
    5.24702407216049, 5.73979291217923, 4.55387689160054, 7.72444664563354, 
    7.91425227874244, 7.11476944836646, 8.24064886337491, 8.56044423341055, 
    6.78558764500793, 10.8861838116069, 2.07944154167984, 5.82894561761021, 
    4.0943445622221, 7.53208814354172, 5.66988092298052, 3.46573590279973, 
    10.7229148990064, 8.02910705461974, 9.23092700558457, 6.42971947803914, 
    11.5129154649202, 7.25700270709207, 6.62007320653036, 9.41995278900604, 
    5.28826703069454, 8.04654935728308, 7.58984151218266, 6.39024066706535, 
    6.04025471127741, 2.77258872223978, 7.62948991639399, 7.85476918349913, 
    7.09007683577609, 5.50125821054473, 6.98100574072173, 5.50533153593236
  )
```



```{r}


y_mean=c(0,0,0)
t_mean=c(0,0,0)

plot(0,0,xlim = c(5.5,7.1),ylim = c(0,12),type = "n", xlab="Time since final vaccine (log scale)", 
     ylab="anti-HBs titre (log scale)", main="Raw data of 106 GHIS infants with mean value(red)")
#cl <- rainbow(5)
HB_log=c()
t_log=c()
nn=c()
for (i in 1:106){
  n=2
  yvec=Yvec1[c(i,i+106)]
  time=tvec1[c(i,i+106)]
  
  
  y_mean[1]=y_mean[1]+yvec[1]
  y_mean[2]=y_mean[2]+yvec[2]
  
  t_mean[1]=t_mean[1]+time[1]
  t_mean[2]=t_mean[2]+time[2]
  HB_log=append(HB_log, yvec)
  t_log=append(t_log, time)
  
  if (i %in% idxn1[213:288])
  {
    loc=match(i,idxn1[213:288])
    yvec=append(yvec,Yvec1[212+loc])
    time=append(time, tvec1[212+loc])
    
    y_mean[3]=y_mean[3]+yvec[3]
    t_mean[3]=t_mean[3]+time[3]
    
    HB_log=append(HB_log, yvec[3])
    t_log=append(t_log, time[3])
    n=3
  }
  
  
  
  else{
    
    HB_log=append(HB_log, NA)
    t_log=append(t_log, 10)
  } 
    
  nn=append(nn, n)

  
  lines(time,yvec,col='grey',type = 'l', lwd=0.5)
}
y_mean[1]=y_mean[1]/106
y_mean[2]=y_mean[2]/106
y_mean[3]=y_mean[3]/76
t_mean[1]=t_mean[1]/106
t_mean[2]=t_mean[2]/106
t_mean[3]=t_mean[3]/76
lines(t_mean,y_mean,col='red',type = 'b', lwd=3, cex=1)


#creating matrix from list for HB and time:
hb_matrix=matrix(HB_log, byrow=TRUE, nrow=106 )
time_matrix=matrix(t_log, byrow=TRUE, nrow=106)
data=list( N = 106,    
           Y =hb_matrix,
           t = time_matrix, 
           y0 = y0,
           n=nn)     #data created 

print ("HB titre matrix:")
head(data$Y)
print ("Time matrix:")
head(data$t)
```

It is also possible to show as histogram the distribution of anti-HB-titre among 106 infants at each test.

```{r}

library(ggplot2)

first_HB_test=data.frame(Yvec1[1:106])
second_HB_test=data.frame(Yvec1[106:212])
third_HB_test = data.frame(Yvec1[213:288])

first_HB_test$n = 'first_HB_test'
second_HB_test$n = 'second_HB_test'
third_HB_test$n = 'third_HB_test'

colnames(first_HB_test)='Test'
colnames(second_HB_test)='Test'
colnames(third_HB_test)='Test'


tests <- rbind(first_HB_test, second_HB_test,third_HB_test)
colnames(tests)[2]='Time'
ggplot(tests, aes(Test, fill = Time)) + geom_density(alpha = 0.2)+ggtitle("Histogram for three HB tests") + xlab("anti-HB-titre")

```


## Model 1

In the first model, we apply random effect on $\alpha_i$ and gradient, since these values have variability in data set, and thus causing overdispersion. We expect that anti-HB-titre is represented as normal distribution, for infant $i$ and at measurement $j$:
$$y_{ij} \sim N(\mu_{ij}, \sigma^2)$$
with mean value represented as:
$$\mu_{ij} = \alpha_i +\beta_i* log(\frac{time_{ij}}{mean(time)})$$
here, we divide $time_{ij}$ by mean value to standartize for numerical stability. For interception $\alpha_i$ and gradient $\beta_i$:
$$\alpha_i \sim N(\alpha_0, \sigma_{\alpha}^2)$$
$$\beta_i \sim N(\beta_0, \sigma_{\beta}^2)$$
Considering priors, we would like the priors to be not too influential for our model, because for we don't have fairly informative priors for this data. Therefore, we model them as follows:
$$\alpha_0, \beta_0 \sim N(0, 10000)$$
and the precisions (1/variance) of all normal distributions have priors as gamma distributions:
$$\sigma^{-2}, \sigma_{\alpha}^{-2}, \sigma_{\beta}^{-2} \sim \Gamma(0.001, 0.001)$$
\begin{figure}[hbt!] 
\centering
\includegraphics[width=3.5in]{C:/Users/admin/Desktop/model1.png} 
\caption{DAG of model 1}
\end{figure}

##Bugs Code of Model 1
\begin{verbatim}
  model
{
   for( i in 1 : N ) {
      for( j in 1 : n[i] ) {
         Y[i , j] ~ dnorm(mu[i , j],tau)
         mu[i , j] <- alpha[i] + beta[i] * (t[i,j] - 6.48)
                    
      }
      alpha[i] ~ dnorm(alpha0,tau.alpha)
      beta[i] ~ dnorm(beta0,tau.beta)
   }
   tau ~ dgamma(0.001,0.001)
   sigma <- 1 / sqrt(tau)
   alpha0 ~ dnorm(0.0,1.0E-4)   
   tau.alpha ~ dgamma(0.001,0.001)
   beta0 ~ dnorm(0.0,1.0E-4)
   tau.beta ~ dgamma(0.001,0.001)
   
}
\end{verbatim}

##Simulation of Model 1

Using JAGS packages, we simulate our model 1. During MCMC, we are interested in five parameters of the model. Chains number is selected as 2, so model parameteres will be checked for convergence twice. Furthermore, number of iterations is chosen as 11000 with 1000 burn-in, which allows to get more stationary results. 
```{r, message=FALSE, warning=FALSE, include=FALSE}
library(rjags)
library(R2jags)
library(ggmcmc)
```

```{r,  message=FALSE, results="hide", warning=FALSE}

initial1=list(alpha0 = 3, beta0 = -2,  tau.alpha = 2, tau.beta = 2, tau = 2)
initial2=list(alpha0 = 4, beta0 = -3,  tau.alpha = 1, tau.beta = 1, tau = 1)
inits=list(initial1, initial2)
set.seed(123)
parameters=c("alpha0","beta0","tau.alpha","tau.beta","tau")
model1=jags(data=data, inits=inits,
            parameters.to.save=parameters,
            model.file="model1.txt",
            n.chains=2,
            n.burnin=1000,
            n.iter=11000)
output=head(model1)

```



After the simulation, $\alpha_0$ has mean value 6.2 and $\beta_0$ is -1.1, which is close to finding of Coursaget. Furthermore, it should be noticed that alpha has higher variance meaning that interception coefficient differs for each infants, while gradient coefficient has higher precision indicating to be a common for all infants. Other important values are Rhat (potential scale reduction factor, for convergence <1.001) and n.eff (measure of effective sample size). The parameter $\tau_{\beta}$ shows non-converged behaviour over iterations according to Rhat. So, let's explore further what happens with each parameter during the simulation.


```{r}
output$BUGSoutput

```


```{r, fig.height=7, fig.width=7}
data_plots = ggs(as.mcmc(model1))
ggs_density(data_plots, greek=T)
ggs_traceplot(data_plots, greek=T)
ggs_running(data_plots, greek=T)
ggs_autocorrelation(data_plots,greek=T)
ggs_crosscorrelation(data_plots, greek=T)

```

Trace-plot shows convergence of most of the parameters to their stationary point after burn-in, except $\tau_{\beta}$, which has some fluctuations. 

Running plot displays how mean value of the parameters changed over iterations. In this plot, we can see that $\tau_{\beta}$ had some difficulties in approaching stationary mean value, while others quickly converged. 

Autocorrelation plot shows how the model parameters are Markovians, meaning no long memory process. In this case, $\tau_{\beta}$ and $\beta_{\alpha}$ has some noticable correlations. 

In crosscorelation plot, all parameters don't have significant correlations between each other. 

## Frequentist Analysis 

In frequency analysis, we apply a linear regresion model for each infant i:

```{r}
sdd=mean(tvec1)
sd_time=data$t-sdd
intercepts <- rep(NA, N)
slopes <- rep(NA, N)
for(i in 1:N){
  fit <- lm(data$Y[i,]~ sd_time[i,])
  intercepts[i] <- coefficients(fit)[[1]]
  slopes[i] <- coefficients(fit)[[2]]
}
mean_alpha <- mean(intercepts)  
mean_beta <- mean(slopes)  

print("Mean alpha:")
print(mean_alpha)
print("Mean beta:")
print(mean_beta)

```

Now, let's see distribution of slopes:

```{r, warning=FALSE}
qplot(slopes, geom="density", xlim=c(-10,10),main="Distribution of gradient (beta) ") 

```

```{r, warning=FALSE}
qplot(intercepts, geom="density", xlim=c(0,15),main="Distribution of intercepts (alpha) ") 

```

```{r, warning=FALSE}
plt <- ggplot(NULL) + aes(x=c(1,6)) +
  theme_grey() +
  labs(x='log(days) since last vaccination',y='log(HB_titre)', title='Bayes vs Freq')+ylim(0,5)
regression = function(x, alpha, beta) alpha+beta*x

plt <- plt + stat_function(fun=regression,
                           args=list(alpha=mean_alpha,beta=mean_beta),
                           size=1, alpha=1,aes(colour = "Frequency"))
plt <- plt + stat_function(fun=regression,
                           args=list(alpha=output$BUGSoutput$mean$alpha0,
                                     beta=output$BUGSoutput$mean$beta0),
                           size=1, alpha=1,aes(colour = "Model 1"))

plt

```






Let's observe relationship of intercepts and the first measurement results:

```{r, warning=FALSE}
qplot(y0, intercepts, geom = c("point", "smooth"), xlab="The first anti-HB-titre test", ylab="Intercepts", main = "The first anti-HB-titre test vs Intercepts", ylim=c(0,15))
```

From this plot, we see that Intercepts have approximately linear relationship with the first anti-HB-titre test results indicating to predisposition of base HB titre to high subequent titres. Considering it, new model can be proposed further.

##Model 2

From the previous plot we saw that there is effect of base HB-titre result on subsequent titres results. Therefore, model 2 will have a following form:
$$y_{ij} \sim N(\mu_{ij}, \sigma^2)$$
$$\mu_{ij}=\alpha_i+\gamma*(y_{i0}-mean(y_0))+\beta_i*log(\frac{time_{ij}}{mean(time)})$$

here $y_{i0}$ is base HB-titre result of infant $i$, and $mean(y_0)$ is the mean value of all base HB-tire results. Subtraction between them is proposed from a point that it will reduce correlation between $\gamma$ and other parameters of the model. The interception and gradient will have the same distribution as in model 1:
$$\alpha_i \sim N(\alpha_0, \sigma_{\alpha}^2)$$
$$\beta_i \sim N(\beta_0, \sigma_{\beta}^2)$$

Their priors will be the same, and also prior for $\gamma$ will have non-informative normal distribution:
$$\alpha_0, \beta_0, \gamma \sim N(0, 10000)$$
$$\sigma^{-2}, \sigma_{\alpha}^{-2}, \sigma_{\beta}^{-2} \sim \Gamma(0.001, 0.001)$$

\begin{figure}[hbt!] 
\centering
\includegraphics[width=3.5in]{C:/Users/admin/Desktop/model2.png} 
\caption{DAG of model 2}
\end{figure}

##Bugs Code of Model 2
\begin{verbatim}
  model
{
   for( i in 1 : N ) {
      for( j in 1 : n[i] ) {
         Y[i , j] ~ dnorm(mu[i , j],tau)
         mu[i , j] <- alpha[i] + beta[i] * (t[i,j] - 6.48) + 
                    gamma * (y0[i] - mean(y0[]))
      }
      alpha[i] ~ dnorm(alpha0,tau.alpha)
      beta[i] ~ dnorm(beta0,tau.beta)
   }
   tau ~ dgamma(0.001,0.001)
   sigma <- 1 / sqrt(tau)
   alpha0 ~ dnorm(0.0,1.0E-4)   
   tau.alpha ~ dgamma(0.001,0.001)
   beta0 ~ dnorm(0.0,1.0E-4)
   tau.beta ~ dgamma(0.001,0.001)
   gamma ~ dnorm(0.0,1.0E-4)
}
\end{verbatim}

##Simulation of Model 2

Simulation parameters will be the same as in model 1, that is, number of iterations is 11000 with 1000 burn-in, and 2 chains are set.

```{r,  message=FALSE, results="hide", warning=FALSE}

initial1_2=list(alpha0 = 3, beta0 = -2,  tau.alpha = 2, tau.beta = 2, tau = 2, gamma=2)
initial2_2=list(alpha0 = 4, beta0 = -3,  tau.alpha = 1, tau.beta = 1, tau = 1, gamma=1)
inits_2=list(initial1_2, initial2_2)
set.seed(111)
parameters_2=c("alpha0","beta0","tau.alpha","tau.beta","tau", "gamma")
model2=jags(data=data, inits=inits_2,
            parameters.to.save=parameters_2,
            model.file="model2.txt",
            n.chains=2,
            n.burnin=1000,
            n.iter=11000)
output2=head(model2)

```

Frome summary of the simulation, $\alpha_0$ and $\beta_0$ haven't changed being equal to the results in model 1. For $\tau_{\beta}$ we still see high precision, while $\tau_{\alpha}$ has higher variance. Furthermore, one important improvement in model 2 is convergence of all values accroding to Rhat, since all of them are equal to 1. Good results also can be seen in n.eff outputs. From this step, let's investigate other results.


```{r}
output2$BUGSoutput

```


```{r, fig.height=8, fig.width=7}
data_plots = ggs(as.mcmc(model2))
ggs_density(data_plots, greek=T)
ggs_traceplot(data_plots, greek=T)
ggs_running(data_plots, greek=T)
ggs_autocorrelation(data_plots,greek=T)
ggs_crosscorrelation(data_plots, greek=T)

```

In trace plot, we can see that all parameters stayed around at their stationary values, despite some fluctuations, especially for $\tau_{beta}$.

In running plot, $\gamma$ and $\tau_{beta}$ had some diffulties in converging at inital steps, but after 10000 iterations, they all converged succefully. 

In autocorreltion plot, all values shows no long memory behaviour.

Significant correlation between parameters of the model are not observed in the last plot. 

Comparing with model 1, all parameters of model 2 perform well.  

##Model 1 vs. Model 2

Since we are going to compare two bayesian models derived from MCMC simulation, good comparison method will be Deviance Information Criterion. This method is based on goodness of fit and model parameters complexity. Defining deviance (goodness of fit) as:

$$D(\theta) =-2log(L(data|\theta))+C$$
here $L(data|\theta)$ is likelihood function for data and model parameters $\theta$. Constant term is ignored during the DIC comparison since they cancel out. When we estimate expected values for model parameters, we have expected deviance as:
$$D(\hat{\theta})=D[E(\theta)]$$

From that, we can calculate the DIC as:
$$DIC=D(\hat{\theta})+2*p_D$$
where $p_D$ is the indicator for complexity of model, which is calculated as:
$$p_D=E_{\theta}[D(\theta)]-D(E[\theta])$$
So, $p_D$ is the penalization term for deviance. Indeed, a model fits easier when it has large number of parameters causing overfitting. Therefore, a model should have effective number of parameters. Finally, by comparing DICs of two models, we can say that a model, which has smaller DIC, is better. 

We are going to check DIC of two models 5 times, in order to be sure about a good performing model. 

```{r,message=FALSE, results="hide", warning=FALSE}
test_number=5
dic1=rep(NA, test_number)
dic2=rep(NA, test_number)

initial1_1=list(alpha0 = 3, beta0 = -2,  tau.alpha = 2, tau.beta = 2, tau = 2)
initial2_1=list(alpha0 = 4, beta0 = -3,  tau.alpha = 1, tau.beta = 1, tau = 1)
inits_1=list(initial1_1, initial2_1)
parameters_1=c("alpha0","beta0","tau.alpha","tau.beta","tau")


initial1_2=list(alpha0 = 3, beta0 = -2,  tau.alpha = 2, tau.beta = 2, tau = 2, gamma=2)
initial2_2=list(alpha0 = 4, beta0 = -3,  tau.alpha = 1, tau.beta = 1, tau = 1, gamma=1)
inits_2=list(initial1_2, initial2_2)
parameters_2=c("alpha0","beta0","tau.alpha","tau.beta","tau", "gamma")

for (n in 1:test_number){
  
  model1=jags(data=data, inits=inits_1,
              parameters.to.save=parameters_1,
              model.file="model1.txt",
              n.chains=2,
              n.burnin=1000,
              n.iter=11000)
  model2=jags(data=data, inits=inits_2,
              parameters.to.save=parameters_2,
              model.file="model2.txt",
              n.chains=2,
              n.burnin=1000,
              n.iter=11000)
  
  output=head(model1)
  output_2=head(model2)
  
  dic1[n] = output$BUGSoutput$DIC
  dic2[n]=output_2$BUGSoutput$DIC
}


```


```{r}

model1=data.frame(dic1)
model2=data.frame(dic2)
model1$n = 'Model 1'
model2$n = 'Model 2'
colnames(model1)='DIC'
colnames(model2)='DIC'
tests <- rbind(model1, model2)
colnames(tests)[2]='DICs'
tests$test_n=c(1,2,3,4,5,1,2,3,4,5)
ggplot(tests, aes(x=test_n, y=DIC, colour = DICs)) + ggtitle("Comparison of models") +geom_line()+ xlab("Test number")



```

From the plot we can see that model 2 shows better result than model 1 for all 5 cases. Therefore, we choose model 2. 



##Ability of fully Bayesian analysis with simulated data

We check ability of fully bayesian analysis to recover model parameters from simulated data from chosen model. For a chosen we selected model 2 from previous part. After that, data will be generated at each iteration, and that data is used for simulation of two models. At each iteration we save corresponding DIC values of two models, to compare it later. 

```{r,message=FALSE, results="hide", warning=FALSE}
iter=5
alpha=6.2
beta=-1.1
gamma=0.7
tau=1
sigma=1/(sqrt(tau))
sdd=mean(tvec1)
y0_mean=mean(y0)

#we need create y0s list:

y02=append(y0, y0)
y03=append(y02, y0)
y03_mat=matrix(y03, byrow=TRUE, nrow=3 )
y03_list=as.vector(y03_mat)

dic1=rep(NA, iter)
dic2=rep(NA, iter)
alpha0_mod_1=rep(NA, iter)
alpha0_mod_2=rep(NA, iter)
beta0_mod_1=rep(NA, iter)
beta0_mod_2=rep(NA, iter)

initial1_1=list(alpha0 = 3, beta0 = -2,  tau.alpha = 2, tau.beta = 2, tau = 2)
initial2_1=list(alpha0 = 4, beta0 = -3,  tau.alpha = 1, tau.beta = 1, tau = 1)
inits_1=list(initial1_1, initial2_1)
parameters_1=c("alpha0","beta0","tau.alpha","tau.beta","tau")

initial1_2=list(alpha0 = 3, beta0 = -2,  tau.alpha = 2, tau.beta = 2, tau = 2, gamma=2)
initial2_2=list(alpha0 = 4, beta0 = -3,  tau.alpha = 1, tau.beta = 1, tau = 1, gamma=1)
inits_2=list(initial1_2, initial2_2)
parameters_2=c("alpha0","beta0","tau.alpha","tau.beta","tau", "gamma")

for (n in 1:iter){
  new_HB=c()
  for (i in 1:length(t_log)){
    mu=alpha+gamma*(y03_list[i]-y0_mean)+beta*(t_log[i]-sdd)
    y=rnorm(1,mean=rnorm(1, mean=mu,sd=sigma))
    new_HB=append(new_HB,y)
  }
  hb_matrix_new=matrix(new_HB, byrow=TRUE, nrow=106 )
  time_matrix_new=matrix(t_log, byrow=TRUE, nrow=106)
  new_data=list( N = 106,    
                 Y =hb_matrix_new,
                 t = time_matrix, 
                 y0 = y0,
                 n=nn)     #data created 
  model1=jags(data=new_data, inits=inits_1,
              parameters.to.save=parameters_1,
              model.file="model1.txt",
              n.chains=2,
              n.burnin=1000,
              n.iter=11000)
  model2=jags(data=new_data, inits=inits_2,
              parameters.to.save=parameters_2,
              model.file="model2.txt",
              n.chains=2,
              n.burnin=1000,
              n.iter=11000)
  
  output=head(model1)
  output_2=head(model2)
  
  dic1[n] = output$BUGSoutput$DIC
  dic2[n]=output_2$BUGSoutput$DIC
  alpha0_mod_1[n]=output$BUGSoutput$mean$alpha0
  alpha0_mod_2[n]=output_2$BUGSoutput$mean$alpha0
  beta0_mod_1[n]=output$BUGSoutput$mean$beta0
  beta0_mod_2[n]=output_2$BUGSoutput$mean$beta0
  
  
}


```

```{r}

n=seq(1,iter)
n=append(n,n)

model1_dic=data.frame(dic1)
model2_dic=data.frame(dic2)
model1_dic$n = 'Model 1'
model2_dic$n = 'Model 2'
colnames(model1_dic)='DIC'
colnames(model2_dic)='DIC'
tests <- rbind(model1_dic, model2_dic)
colnames(tests)[2]='DICs'
tests$test_n=n
ggplot(tests, aes(x=test_n, y=DIC, colour = DICs)) + ggtitle("Comparison of models") +geom_line()+ xlab("Test number")


model1_alpha=data.frame(alpha0_mod_1)
model2_alpha=data.frame(alpha0_mod_2)
model1_alpha$n = 'Model 1'
model2_alpha$n = 'Model 2'
colnames(model1_alpha)='Alpha0'
colnames(model2_alpha)='Alpha0'
tests <- rbind(model1_alpha, model2_alpha)
colnames(tests)[2]='Alphas'
tests$test_n=n
ggplot(tests, aes(x=test_n, y=Alpha0, colour = Alphas)) + ggtitle("Parameters recover, Interception") +geom_line()+ xlab("Test number")+geom_hline(yintercept=6.2)

model1_beta=data.frame(beta0_mod_1)
model2_beta=data.frame(beta0_mod_2)
model1_beta$n = 'Model 1'
model2_beta$n = 'Model 2'
colnames(model1_beta)='Beta0'
colnames(model2_beta)='Beta0'
tests <- rbind(model1_beta, model2_beta)
colnames(tests)[2]='Betas'
tests$test_n=n
ggplot(tests, aes(x=test_n, y=Beta0, colour = Betas)) + ggtitle("Parameters recover, gradient") +geom_line()+ xlab("Test number")+geom_hline(yintercept=-1.1)





```

##Final evaluation


Here, we evaluate models with finding of Coursaget et al. graphically. According to the study, model parameters are $\alpha_{mean}=5.94$,  $\beta_{mean}=-0.95$ and $\gamma_{mean}=0.6$. Now, considering the range for $log(time) \in [5.5, 7]$,  let's plot lines based on mean values:


```{r}

#let's take a base log titre value for our model 2 and Coursaget model, computing mean 
#at time log(5.5) for model 1 and frequency analysis:
tm=mean(tvec1)
y1=mean_alpha+mean_beta*(5.5-tm)
y2=6.2-1.1*(5.5-tm)
y0_test=(y1+y2)/2


plt <- ggplot(NULL) + aes(x=c(5.5,7)) +
  theme_classic() +
  labs(x='log(days) since last vaccination',y='log(HB_titre)', title='Comparison of all models')
regression = function(x, alpha, beta) alpha+beta*(x-tm)


plt <- plt + stat_function(fun=regression,
                           args=list(alpha=mean_alpha,beta=mean_beta),
                           size=1, alpha=1,aes(colour = "Frequency")) #from freq analysis
plt <- plt + stat_function(fun=regression,
                           args=list(alpha=6.2,
                                     beta=-1.1),
                           size=1, alpha=1,aes(color='model 1'))   #from model 1
plt <- plt + stat_function(fun=regression,
                           args=list(alpha=6.2+0.7*(y0_test-mean(y0)),
                                     beta=-1.1),
                           size=1, alpha=1,aes(color='model 2'))  #from model 2
plt <- plt + stat_function(fun=regression,
                           args=list(alpha=5.94+0.6*(y0_test-mean(y0)),
                                     beta=-0.95),
                           size=1, alpha=1,aes(color='Coursaget'))  #from model 2
plt


```



##Reference
1. Coursaget et al. (1991). "Scheduling of revaccination against hepatitis B". The Lancet vol 337
2. Spiegelhalter et al.(1996). "Markov Chain Monte Carlo in Practice". Chapman and Hall
3. Tardella L. (2018). "Course: Statistical Methods for Data Science  II, lecture notes". 






